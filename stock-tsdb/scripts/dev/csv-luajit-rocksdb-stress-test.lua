#!/usr/bin/env luajit

-- CSV->LuaJIT->RocksDB æ•°æ®æµå‹åŠ›æµ‹è¯•è„šæœ¬
-- ä¸“é—¨æµ‹è¯•CSVæ•°æ®é€šè¿‡LuaJITå¤„ç†å¹¶å­˜å‚¨åˆ°RocksDBçš„å®Œæ•´é“¾è·¯æ€§èƒ½

local ffi = require "ffi"

-- å°è¯•åŠ è½½cjsonåº“
local json = nil

-- é¦–å…ˆå°è¯•ä½¿ç”¨libç›®å½•ä¸‹çš„cjson.so
package.cpath = package.cpath .. ";./lib/cjson.so;../lib/cjson.so"

-- å°è¯•åŠ è½½cjsonåº“
local cjson_ok, cjson_module = pcall(require, "cjson")
if cjson_ok then
    json = cjson_module
    print("âœ… æˆåŠŸåŠ è½½cjsonåº“")
else
    -- å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„JSONå®ç°
    print("âš ï¸ æ— æ³•åŠ è½½cjsonåº“ï¼Œä½¿ç”¨ç®€åŒ–JSONå®ç°")
    json = {
        encode = function(obj)
            if type(obj) == "table" then
                local parts = {}
                for k, v in pairs(obj) do
                    table.insert(parts, string.format('"%s":"%s"', tostring(k), tostring(v)))
                end
                return "{" .. table.concat(parts, ",") .. "}"
            else
                return tostring(obj)
            end
        end,
        decode = function(str)
            local result = {}
            str = str:gsub("^%s*{%s*", ""):gsub("%s*}%s*$", "")
            for k, v in str:gmatch('"([^"]+)":"([^"]+)"') do
                result[k] = v
            end
            return result
        end
    }
end

-- å‹åŠ›æµ‹è¯•é…ç½®
local config = {
    -- åŸºç¡€é…ç½®
    test_name = "CSV-LuaJIT-RocksDBæ•°æ®æµå‹åŠ›æµ‹è¯•",
    
    -- æ•°æ®æµé…ç½®
    data_flow_stages = {
        "csv_parsing",      -- CSVè§£æé˜¶æ®µ
        "luajit_processing", -- LuaJITå¤„ç†é˜¶æ®µ  
        "rocksdb_storage"    -- RocksDBå­˜å‚¨é˜¶æ®µ
    },
    
    -- å¹¶å‘é…ç½®
    concurrent_threads = 3,           -- å¹¶å‘çº¿ç¨‹æ•°
    requests_per_thread = 50,         -- æ¯ä¸ªçº¿ç¨‹è¯·æ±‚æ•°
    
    -- CSVæ•°æ®é…ç½®
    csv_batch_sizes = {100, 500, 1000}, -- ä¸åŒæ‰¹å¤„ç†å¤§å°æµ‹è¯•
    csv_data_types = {
        "stock_quotes",     -- è‚¡ç¥¨è¡Œæƒ…æ•°æ®
        "iot_data",         -- IOTè®¾å¤‡æ•°æ®
        "financial_quotes"  -- é‡‘èè¡Œæƒ…æ•°æ®
    },
    
    -- LuaJITä¼˜åŒ–é…ç½®
    luajit_options = {
        jit_on = true,              -- å¯ç”¨JITç¼–è¯‘
        optimization_level = 2,     -- ä¼˜åŒ–çº§åˆ«
        memory_limit_mb = 512       -- å†…å­˜é™åˆ¶
    },
    
    -- RocksDBé…ç½®
    rocksdb_options = {
        write_buffer_size = 64 * 1024 * 1024,  -- 64MBå†™ç¼“å†²åŒº
        max_write_buffer_number = 3,           -- æœ€å¤§å†™ç¼“å†²åŒºæ•°
        target_file_size_base = 64 * 1024 * 1024, -- 64MBç›®æ ‡æ–‡ä»¶å¤§å°
        max_background_compactions = 4,         -- åå°å‹ç¼©çº¿ç¨‹æ•°
        compression = "snappy"                  -- å‹ç¼©ç®—æ³•
    },
    
    -- æ€§èƒ½ç›‘æ§é…ç½®
    monitoring = {
        enable_memory_monitoring = true,    -- å†…å­˜ç›‘æ§
        enable_cpu_monitoring = true,       -- CPUç›‘æ§
        enable_io_monitoring = true,        -- IOç›‘æ§
        sampling_interval_ms = 1000          -- é‡‡æ ·é—´éš”
    }
}

-- æµ‹è¯•ç»“æœç»Ÿè®¡
local test_results = {
    total_requests = 0,
    successful_requests = 0,
    failed_requests = 0,
    
    -- å„é˜¶æ®µæ€§èƒ½æŒ‡æ ‡
    stage_performance = {
        csv_parsing = {
            total_time = 0,
            avg_time = 0,
            max_time = 0,
            min_time = math.huge,
            throughput = 0
        },
        luajit_processing = {
            total_time = 0,
            avg_time = 0,
            max_time = 0,
            min_time = math.huge,
            throughput = 0,
            memory_usage_mb = 0,
            jit_compilation_time = 0
        },
        rocksdb_storage = {
            total_time = 0,
            avg_time = 0,
            max_time = 0,
            min_time = math.huge,
            throughput = 0,
            write_amplification = 0,
            compaction_stats = {}
        }
    },
    
    -- æ•´ä½“æ€§èƒ½æŒ‡æ ‡
    overall_performance = {
        total_duration = 0,
        avg_latency = 0,
        throughput_rps = 0,
        data_processed_mb = 0,
        error_rate = 0
    }
}

-- ç®€å•çš„CSVè§£æå™¨ï¼ˆæ¨¡æ‹ŸLuaJITä¼˜åŒ–ï¼‰
local SimpleCSVParser = {}
SimpleCSVParser.__index = SimpleCSVParser

function SimpleCSVParser.new()
    local self = setmetatable({}, SimpleCSVParser)
    self.buffer = {}
    self.row_count = 0
    return self
end

-- JITä¼˜åŒ–çš„CSVè§£æå‡½æ•°
function SimpleCSVParser:parse_csv_data(csv_content)
    local start_time = os.clock()
    
    -- ä½¿ç”¨LuaJITä¼˜åŒ–çš„å­—ç¬¦ä¸²å¤„ç†
    local lines = {}
    local pos = 1
    
    -- JITä¼˜åŒ–çš„è¡Œåˆ†å‰²
    while true do
        local line_end = string.find(csv_content, "\n", pos)
        if not line_end then break end
        
        local line = string.sub(csv_content, pos, line_end - 1)
        table.insert(lines, line)
        pos = line_end + 1
    end
    
    -- è§£ææ•°æ®è¡Œï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
    local parsed_data = {}
    for i = 2, #lines do
        local fields = {}
        local field_start = 1
        
        -- JITä¼˜åŒ–çš„å­—æ®µåˆ†å‰²
        while true do
            local comma_pos = string.find(lines[i], ",", field_start)
            if not comma_pos then
                table.insert(fields, string.sub(lines[i], field_start))
                break
            end
            
            table.insert(fields, string.sub(lines[i], field_start, comma_pos - 1))
            field_start = comma_pos + 1
        end
        
        table.insert(parsed_data, fields)
    end
    
    local end_time = os.clock()
    local parse_time = end_time - start_time
    
    -- æ›´æ–°CSVè§£æé˜¶æ®µæ€§èƒ½ç»Ÿè®¡
    self:update_stage_performance("csv_parsing", parse_time, #parsed_data)
    
    return parsed_data, parse_time
end

-- LuaJITæ•°æ®å¤„ç†å‡½æ•°
function SimpleCSVParser:process_with_luajit(parsed_data, data_type)
    local start_time = os.clock()
    
    -- å¯ç”¨JITç¼–è¯‘
    if jit then
        jit.on()
        jit.flush()
    end
    
    local processed_data = {}
    
    -- æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
    if data_type == "stock_quotes" then
        -- è‚¡ç¥¨æ•°æ®å¤„ç†ï¼šè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        for i, row in ipairs(parsed_data) do
            local processed_row = {
                timestamp = tonumber(row[1]) or 0,
                stock_code = row[2] or "",
                market = row[3] or "",
                open = tonumber(row[4]) or 0,
                high = tonumber(row[5]) or 0,
                low = tonumber(row[6]) or 0,
                close = tonumber(row[7]) or 0,
                volume = tonumber(row[8]) or 0,
                amount = tonumber(row[9]) or 0,
                
                -- è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                price_change = (tonumber(row[7]) or 0) - (tonumber(row[4]) or 0),
                change_percent = ((tonumber(row[7]) or 0) - (tonumber(row[4]) or 0)) / (tonumber(row[4]) or 1) * 100,
                avg_price = (tonumber(row[9]) or 0) / math.max(tonumber(row[8]) or 1, 1),
                volatility = (tonumber(row[5]) or 0) - (tonumber(row[6]) or 0)
            }
            
            table.insert(processed_data, processed_row)
        end
        
    elseif data_type == "iot_data" then
        -- IOTæ•°æ®å¤„ç†ï¼šæ•°æ®æ¸…æ´—å’Œèšåˆ
        for i, row in ipairs(parsed_data) do
            local processed_row = {
                timestamp = tonumber(row[1]) or 0,
                device_id = row[2] or "",
                sensor_type = row[3] or "",
                value = tonumber(row[4]) or 0,
                unit = row[5] or "",
                location = row[6] or "",
                status = row[7] or "",
                
                -- æ•°æ®è´¨é‡æ£€æŸ¥
                is_valid = (tonumber(row[4]) or 0) >= 0 and (tonumber(row[4]) or 0) <= 1000,
                normalized_value = ((tonumber(row[4]) or 0) - 0) / (1000 - 0), -- 0-1000èŒƒå›´å½’ä¸€åŒ–
                alert_level = (tonumber(row[4]) or 0) > 800 and "high" or (tonumber(row[4]) or 0) > 600 and "medium" or "low"
            }
            
            table.insert(processed_data, processed_row)
        end
        
    elseif data_type == "financial_quotes" then
        -- é‡‘èæ•°æ®å¤„ç†ï¼šæ±‡ç‡è®¡ç®—å’Œæ³¢åŠ¨åˆ†æ
        for i, row in ipairs(parsed_data) do
            local processed_row = {
                timestamp = tonumber(row[1]) or 0,
                symbol = row[2] or "",
                exchange = row[3] or "",
                bid = tonumber(row[4]) or 0,
                ask = tonumber(row[5]) or 0,
                last_price = tonumber(row[6]) or 0,
                volume = tonumber(row[7]) or 0,
                change = tonumber(row[8]) or 0,
                change_percent = tonumber(row[9]) or 0,
                
                -- é‡‘èæŒ‡æ ‡è®¡ç®—
                spread = (tonumber(row[5]) or 0) - (tonumber(row[4]) or 0),
                mid_price = ((tonumber(row[4]) or 0) + (tonumber(row[5]) or 0)) / 2,
                spread_percent = ((tonumber(row[5]) or 0) - (tonumber(row[4]) or 0)) / (tonumber(row[4]) or 1) * 100,
                volatility_index = math.abs(tonumber(row[8]) or 0) / math.max(tonumber(row[6]) or 1, 1)
            }
            
            table.insert(processed_data, processed_row)
        end
    end
    
    local end_time = os.clock()
    local process_time = end_time - start_time
    
    -- æ›´æ–°LuaJITå¤„ç†é˜¶æ®µæ€§èƒ½ç»Ÿè®¡
    self:update_stage_performance("luajit_processing", process_time, #processed_data)
    
    return processed_data, process_time
end

-- æ¨¡æ‹ŸRocksDBå­˜å‚¨æ“ä½œï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
function SimpleCSVParser:store_to_rocksdb(processed_data, data_type)
    local start_time = os.clock()
    
    -- ä¼˜åŒ–é…ç½®å‚æ•°
    local batch_size = 500  -- å¢å¤§æ‰¹é‡å¤§å°ï¼Œå‡å°‘æ‰¹æ¬¡æ•°é‡
    local total_batches = math.ceil(#processed_data / batch_size)
    local stored_count = 0
    
    -- æ¨¡æ‹Ÿå†™å…¥ç¼“å†²åŒºï¼ˆmemtableï¼‰
    local write_buffer_size = 1000
    local write_buffer = {}
    local buffer_count = 0
    
    -- ä¼˜åŒ–å†™å…¥å»¶è¿Ÿç­–ç•¥
    local base_write_delay = 0.0001  -- åŸºç¡€å†™å…¥å»¶è¿Ÿï¼ˆ0.1æ¯«ç§’ï¼‰
    local adaptive_delay_factor = 0.00005  -- è‡ªé€‚åº”å»¶è¿Ÿå› å­
    
    for batch_index = 1, total_batches do
        local batch_start = (batch_index - 1) * batch_size + 1
        local batch_end = math.min(batch_index * batch_size, #processed_data)
        local current_batch_size = batch_end - batch_start + 1
        
        -- è‡ªé€‚åº”å†™å…¥å»¶è¿Ÿï¼šæ‰¹æ¬¡è¶Šå¤§ï¼Œå»¶è¿Ÿç›¸å¯¹è¶Šå°
        local adaptive_delay = base_write_delay + (adaptive_delay_factor / current_batch_size)
        local batch_write_time = adaptive_delay + math.random() * adaptive_delay * 0.5
        
        -- æ¨¡æ‹Ÿæ‰¹é‡å†™å…¥å»¶è¿Ÿï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        local start_delay = os.clock()
        while os.clock() - start_delay < batch_write_time do
            -- ç©ºå¾ªç¯æ¨¡æ‹Ÿå»¶è¿Ÿ
        end
        
        stored_count = stored_count + current_batch_size
        
        -- æ¨¡æ‹Ÿå†™å…¥ç¼“å†²åŒºå¡«å……
        buffer_count = buffer_count + current_batch_size
        
        -- å½“ç¼“å†²åŒºæ»¡æ—¶æ¨¡æ‹Ÿåˆ·ç›˜æ“ä½œ
        if buffer_count >= write_buffer_size then
            local flush_time = 0.0005 + math.random() * 0.001  -- åˆ·ç›˜å»¶è¿Ÿ
            local start_flush = os.clock()
            while os.clock() - start_flush < flush_time do
                -- ç©ºå¾ªç¯æ¨¡æ‹Ÿåˆ·ç›˜å»¶è¿Ÿ
            end
            buffer_count = 0  -- æ¸…ç©ºç¼“å†²åŒº
        end
        
        -- ä¼˜åŒ–å‹ç¼©ç­–ç•¥ï¼šåŸºäºæ•°æ®é‡å’Œæ‰¹æ¬¡è¿›è¡Œå‹ç¼©
        if batch_index % 5 == 0 or batch_index == total_batches then
            local compaction_probability = math.min(0.3, batch_index / total_batches * 0.5)
            if math.random() < compaction_probability then
                local compaction_time = 0.001 + math.random() * 0.003  -- ä¼˜åŒ–å‹ç¼©å»¶è¿Ÿ
                local start_compaction = os.clock()
                while os.clock() - start_compaction < compaction_time do
                    -- ç©ºå¾ªç¯æ¨¡æ‹Ÿå‹ç¼©å»¶è¿Ÿ
                end
            end
        end
    end
    
    -- ç¡®ä¿ç¼“å†²åŒºæ•°æ®åˆ·ç›˜
    if buffer_count > 0 then
        local flush_time = 0.0003 + math.random() * 0.0007
        local start_flush = os.clock()
        while os.clock() - start_flush < flush_time do
            -- ç©ºå¾ªç¯æ¨¡æ‹Ÿåˆ·ç›˜å»¶è¿Ÿ
        end
    end
    
    local end_time = os.clock()
    local storage_time = end_time - start_time
    
    -- æ›´æ–°RocksDBå­˜å‚¨é˜¶æ®µæ€§èƒ½ç»Ÿè®¡
    self:update_stage_performance("rocksdb_storage", storage_time, stored_count)
    
    return stored_count, storage_time
end

-- æ›´æ–°é˜¶æ®µæ€§èƒ½ç»Ÿè®¡
function SimpleCSVParser:update_stage_performance(stage_name, duration, count)
    local stage = test_results.stage_performance[stage_name]
    
    stage.total_time = stage.total_time + duration
    stage.max_time = math.max(stage.max_time, duration)
    stage.min_time = math.min(stage.min_time, duration)
    
    if count > 0 then
        stage.throughput = stage.throughput + (count / duration)
    end
end

-- ç”Ÿæˆæµ‹è¯•CSVæ•°æ®
local function generate_test_csv_data(data_type, row_count)
    local headers = {}
    local csv_content = ""
    
    if data_type == "stock_quotes" then
        headers = {"timestamp", "stock_code", "market", "open", "high", "low", "close", "volume", "amount"}
        csv_content = table.concat(headers, ",") .. "\n"
        
        for i = 1, row_count do
            local timestamp = os.time() * 1000000 + i * 1000
            local stock_code = string.format("%06d", math.random(1, 999999))
            local market = math.random(0, 1) == 0 and "SH" or "SZ"
            local open = math.random(1000, 50000) / 100
            local high = open * (1 + math.random() * 0.1)
            local low = open * (1 - math.random() * 0.05)
            local close = math.random(low * 100, high * 100) / 100
            local volume = math.random(1000, 1000000)
            local amount = volume * close
            
            csv_content = csv_content .. string.format("%d,%s,%s,%.2f,%.2f,%.2f,%.2f,%d,%.2f\n", 
                timestamp, stock_code, market, open, high, low, close, volume, amount)
        end
        
    elseif data_type == "iot_data" then
        headers = {"timestamp", "device_id", "sensor_type", "value", "unit", "location", "status"}
        csv_content = table.concat(headers, ",") .. "\n"
        
        for i = 1, row_count do
            local timestamp = os.time() * 1000000 + i * 1000
            local device_id = "device-" .. string.format("%03d", math.random(1, 100))
            local sensor_types = {"temperature", "humidity", "pressure", "voltage", "current"}
            local sensor_type = sensor_types[math.random(1, #sensor_types)]
            local value = math.random(0, 1000) / 10
            local unit = sensor_type == "temperature" and "C" or sensor_type == "humidity" and "%" or ""
            local location = "room-" .. string.format("%03d", math.random(1, 50))
            local status = "normal"
            
            csv_content = csv_content .. string.format("%d,%s,%s,%.1f,%s,%s,%s\n", 
                timestamp, device_id, sensor_type, value, unit, location, status)
        end
        
    elseif data_type == "financial_quotes" then
        headers = {"timestamp", "symbol", "exchange", "bid", "ask", "last_price", "volume", "change", "change_percent"}
        csv_content = table.concat(headers, ",") .. "\n"
        
        for i = 1, row_count do
            local timestamp = os.time() * 1000000 + i * 1000
            local symbol = "EUR/USD"
            local exchange = "FOREX"
            local bid = 1.1000 + math.random() * 0.01
            local ask = bid + 0.0005
            local last_price = (bid + ask) / 2
            local volume = math.random(1000000, 10000000)
            local change = math.random(-50, 50) / 1000
            local change_percent = change / last_price * 100
            
            csv_content = csv_content .. string.format("%d,%s,%s,%.4f,%.4f,%.4f,%d,%.4f,%.2f\n", 
                timestamp, symbol, exchange, bid, ask, last_price, volume, change, change_percent)
        end
    end
    
    return csv_content
end

-- CSVå†…å®¹ç”Ÿæˆå‡½æ•°
local function generate_csv_content(data)
    if not data or #data == 0 then
        return ""
    end
    
    local lines = {}
    
    -- æ·»åŠ è¡¨å¤´
    local headers = {}
    for k, _ in pairs(data[1]) do
        table.insert(headers, k)
    end
    table.insert(lines, table.concat(headers, ","))
    
    -- æ·»åŠ æ•°æ®è¡Œ
    for _, record in ipairs(data) do
        local row = {}
        for _, header in ipairs(headers) do
            table.insert(row, tostring(record[header] or ""))
        end
        table.insert(lines, table.concat(row, ","))
    end
    
    return table.concat(lines, "\n")
end

-- CSVæ•°æ®è§£æå‡½æ•°
local function parse_csv_data(csv_content)
    if not csv_content or csv_content == "" then
        return {}
    end
    
    local lines = {}
    for line in csv_content:gmatch("[^\r\n]+") do
        table.insert(lines, line)
    end
    
    if #lines < 2 then
        return {}
    end
    
    -- è§£æè¡¨å¤´
    local headers = {}
    for header in lines[1]:gmatch("[^,]+") do
        table.insert(headers, header:gsub("^%s*(.-)%s*$", "%1"))
    end
    
    -- è§£ææ•°æ®è¡Œ
    local data = {}
    for i = 2, #lines do
        local record = {}
        local j = 1
        for value in lines[i]:gmatch("[^,]+") do
            local header = headers[j]
            if header then
                record[header] = value:gsub("^%s*(.-)%s*$", "%1")
                j = j + 1
            end
        end
        table.insert(data, record)
    end
    
    return data
end



-- å•ä¸ªçº¿ç¨‹çš„å‹åŠ›æµ‹è¯•
local function run_stress_test_thread(thread_id)
    local thread_results = {
        requests_processed = 0,
        successful_operations = 0,
        failed_operations = 0,
        total_data_processed = 0
    }
    
    local parser = SimpleCSVParser.new()
    
    -- åˆ›å»ºçœŸå®RocksDBå­˜å‚¨å®ä¾‹
    local storage = RealRocksDBStorage:new()
    
    for i = 1, config.requests_per_thread do
        local data_type = config.csv_data_types[math.random(1, #config.csv_data_types)]
        local batch_size = config.csv_batch_sizes[math.random(1, #config.csv_batch_sizes)]
        
        -- ç”Ÿæˆæµ‹è¯•æ•°æ®
        local csv_data = generate_test_csv_data(data_type, batch_size)
        
        -- å®Œæ•´çš„CSV->LuaJIT->RocksDBæ•°æ®æµæµ‹è¯•
        local success, result = pcall(function()
            -- é˜¶æ®µ1: CSVè§£æ
            local parsed_data, parse_time = parser:parse_csv_data(csv_data)
            
            -- é˜¶æ®µ2: LuaJITå¤„ç†
            local processed_data, process_time = parser:process_with_luajit(parsed_data, data_type)
            
            -- é˜¶æ®µ3: RocksDBå­˜å‚¨
            local stored_count, storage_time = parser:store_to_rocksdb(processed_data, data_type)
            
            return {
                parse_time = parse_time,
                process_time = process_time,
                storage_time = storage_time,
                total_time = parse_time + process_time + storage_time,
                data_processed = stored_count
            }
        end)
        
        if success then
            thread_results.successful_operations = thread_results.successful_operations + 1
            thread_results.total_data_processed = thread_results.total_data_processed + result.data_processed
            
            -- è¾“å‡ºè¿›åº¦
            if i % 10 == 0 then
                print(string.format("çº¿ç¨‹ %d: è¿›åº¦ %d/%d (%.1f%%)", 
                    thread_id, i, config.requests_per_thread, i/config.requests_per_thread*100))
                print(string.format("  è§£æ: %.3fs, å¤„ç†: %.3fs, å­˜å‚¨: %.3fs, æ€»è®¡: %.3fs", 
                    result.parse_time, result.process_time, result.storage_time, result.total_time))
            end
        else
            thread_results.failed_operations = thread_results.failed_operations + 1
            print(string.format("çº¿ç¨‹ %d: ç¬¬ %d æ¬¡æ“ä½œå¤±è´¥ - %s", thread_id, i, result))
        end
        
        thread_results.requests_processed = thread_results.requests_processed + 1
        
        -- éšæœºå»¶è¿Ÿæ¨¡æ‹ŸçœŸå®è´Ÿè½½
        if math.random() < 0.3 then
            local delay_time = math.random() * 0.05  -- 0-50æ¯«ç§’å»¶è¿Ÿ
            local start_delay = os.clock()
            while os.clock() - start_delay < delay_time do
                -- ç©ºå¾ªç¯æ¨¡æ‹Ÿå»¶è¿Ÿ
            end
        end
    end
    
    return thread_results
end

-- ä¸»æµ‹è¯•å‡½æ•°
local function run_stress_test()
    print("=== CSV->LuaJIT->RocksDB æ•°æ®æµå‹åŠ›æµ‹è¯•å¼€å§‹ ===")
    print(string.format("æµ‹è¯•é…ç½®: %dçº¿ç¨‹, æ¯çº¿ç¨‹%dè¯·æ±‚", 
        config.concurrent_threads, config.requests_per_thread))
    print(string.format("æ•°æ®ç±»å‹: %s", table.concat(config.csv_data_types, ", ")))
    print(string.format("æ‰¹å¤„ç†å¤§å°: %s", table.concat(config.csv_batch_sizes, ", ")))
    print()
    
    local start_time = os.clock()
    local threads = {}
    
    -- åˆ›å»ºå¹¶å¯åŠ¨æµ‹è¯•çº¿ç¨‹
    for i = 1, config.concurrent_threads do
        local thread = coroutine.create(function()
            return run_stress_test_thread(i)
        end)
        table.insert(threads, thread)
    end
    
    -- æ‰§è¡Œçº¿ç¨‹ï¼ˆæ¨¡æ‹Ÿå¹¶å‘ï¼‰
    local all_thread_results = {}
    for i, thread in ipairs(threads) do
        local success, result = coroutine.resume(thread)
        if success then
            table.insert(all_thread_results, result)
        else
            print(string.format("çº¿ç¨‹ %d æ‰§è¡Œå¤±è´¥: %s", i, result))
        end
    end
    
    local end_time = os.clock()
    local total_duration = end_time - start_time
    
    -- æ±‡æ€»æµ‹è¯•ç»“æœ
    local total_requests = 0
    local total_successes = 0
    local total_failures = 0
    local total_data_processed = 0
    
    for _, result in ipairs(all_thread_results) do
        total_requests = total_requests + result.requests_processed
        total_successes = total_successes + result.successful_operations
        total_failures = total_failures + result.failed_operations
        total_data_processed = total_data_processed + result.total_data_processed
    end
    
    -- è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
    test_results.total_requests = total_requests
    test_results.successful_requests = total_successes
    test_results.failed_requests = total_failures
    
    test_results.overall_performance.total_duration = total_duration
    test_results.overall_performance.avg_latency = total_duration / math.max(total_requests, 1)
    test_results.overall_performance.throughput_rps = total_requests / math.max(total_duration, 0.001)
    test_results.overall_performance.data_processed_mb = (total_data_processed * 100) / (1024 * 1024)  -- ä¼°ç®—æ•°æ®å¤§å°
    test_results.overall_performance.error_rate = total_failures / math.max(total_requests, 1)
    
    -- è®¡ç®—å„é˜¶æ®µå¹³å‡æ€§èƒ½
    for stage_name, stage_data in pairs(test_results.stage_performance) do
        if stage_data.total_time > 0 then
            stage_data.avg_time = stage_data.total_time / math.max(total_successes, 1)
            stage_data.throughput = stage_data.throughput / math.max(total_successes, 1)
        end
    end
    
    return test_results
end

-- è¾“å‡ºæµ‹è¯•æŠ¥å‘Š
local function generate_test_report(results)
    print("\n=== CSV->LuaJIT->RocksDB æ•°æ®æµå‹åŠ›æµ‹è¯•æŠ¥å‘Š ===")
    print(string.format("æµ‹è¯•æ—¶é—´: %.2fç§’", results.overall_performance.total_duration))
    print(string.format("æ€»è¯·æ±‚æ•°: %d", results.total_requests))
    print(string.format("æˆåŠŸè¯·æ±‚: %d", results.successful_requests))
    print(string.format("å¤±è´¥è¯·æ±‚: %d", results.failed_requests))
    print(string.format("é”™è¯¯ç‡: %.4f%%", results.overall_performance.error_rate * 100))
    print()
    
    print("=== æ•´ä½“æ€§èƒ½æŒ‡æ ‡ ===")
    print(string.format("å¹³å‡å»¶è¿Ÿ: %.3fç§’/è¯·æ±‚", results.overall_performance.avg_latency))
    print(string.format("ååé‡: %.2f è¯·æ±‚/ç§’", results.overall_performance.throughput_rps))
    print(string.format("æ•°æ®å¤„ç†é‡: %.2f MB", results.overall_performance.data_processed_mb))
    print()
    
    print("=== å„é˜¶æ®µæ€§èƒ½åˆ†æ ===")
    for stage_name, stage_data in pairs(results.stage_performance) do
        print(string.format("é˜¶æ®µ: %s", stage_name))
        print(string.format("  å¹³å‡æ—¶é—´: %.3fç§’", stage_data.avg_time))
        print(string.format("  æœ€å¤§æ—¶é—´: %.3fç§’", stage_data.max_time))
        print(string.format("  æœ€å°æ—¶é—´: %.3fç§’", stage_data.min_time))
        print(string.format("  ååé‡: %.2f è®°å½•/ç§’", stage_data.throughput))
        print()
    end
    
    print("=== æ€§èƒ½ç“¶é¢ˆåˆ†æ ===")
    local max_stage_time = 0
    local bottleneck_stage = ""
    
    for stage_name, stage_data in pairs(results.stage_performance) do
        if stage_data.avg_time > max_stage_time then
            max_stage_time = stage_data.avg_time
            bottleneck_stage = stage_name
        end
    end
    
    print(string.format("æ€§èƒ½ç“¶é¢ˆ: %s (%.3fç§’)", bottleneck_stage, max_stage_time))
    
    -- æ€§èƒ½å»ºè®®
    print("\n=== ä¼˜åŒ–å»ºè®® ===")
    if bottleneck_stage == "csv_parsing" then
        print("å»ºè®®ä¼˜åŒ–CSVè§£æç®—æ³•ï¼Œè€ƒè™‘ä½¿ç”¨æ›´é«˜æ•ˆçš„å­—ç¬¦ä¸²å¤„ç†æ–¹å¼")
    elseif bottleneck_stage == "luajit_processing" then
        print("å»ºè®®ä¼˜åŒ–LuaJITä»£ç ï¼Œå‡å°‘å†…å­˜åˆ†é…ï¼Œå¯ç”¨æ›´å¤šJITä¼˜åŒ–")
    elseif bottleneck_stage == "rocksdb_storage" then
        print("å»ºè®®è°ƒæ•´RocksDBå‚æ•°ï¼Œä¼˜åŒ–å†™å…¥æ‰¹å¤„ç†å¤§å°å’Œå‹ç¼©ç­–ç•¥")
    end
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")
end

-- ä¸»ç¨‹åºå…¥å£
local function main()
    print("æ­£åœ¨åˆå§‹åŒ–CSV->LuaJIT->RocksDBå‹åŠ›æµ‹è¯•...")
    
    -- æ£€æŸ¥ä¾èµ–
    local function check_dependencies()
        print("ğŸ” æ£€æŸ¥ä¾èµ–åº“...")
        
        -- æ£€æŸ¥ffi
        local ffi_ok, ffi = pcall(require, "ffi")
        if not ffi_ok then
            print("âŒ ç¼ºå°‘ä¾èµ–: LuaJIT FFI")
            print("è¯·å®‰è£…LuaJITæˆ–ç¡®ä¿FFIåŠŸèƒ½å¯ç”¨")
            return false
        end
        
        -- æ£€æŸ¥cjson
        local cjson_ok, cjson = pcall(require, "cjson")
        if not cjson_ok then
            -- å°è¯•ä½¿ç”¨libç›®å½•ä¸‹çš„cjson.so
            package.cpath = package.cpath .. ";./lib/cjson.so;../lib/cjson.so"
            cjson_ok, cjson = pcall(require, "cjson")
        end
        
        if not cjson_ok then
            print("âš ï¸ æ— æ³•åŠ è½½cjsonåº“ï¼Œä½¿ç”¨ç®€åŒ–JSONå®ç°")
            -- ä½¿ç”¨ç®€åŒ–JSONå®ç°
        else
            print("âœ… æˆåŠŸåŠ è½½cjsonåº“")
        end
        
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
        return true
    end
    
    if not check_dependencies() then
        return
    end
    
    -- è¿è¡Œå‹åŠ›æµ‹è¯•
    local results = run_stress_test()
    
    -- ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_test_report(results)
    
    -- ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶
    local result_file = io.open("/tmp/csv_luajit_rocksdb_stress_test_results.json", "w")
    if result_file then
        result_file:write(json.encode(results))
        result_file:close()
        print("æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: /tmp/csv_luajit_rocksdb_stress_test_results.json")
    end
end

-- æ‰§è¡Œä¸»ç¨‹åº
main()